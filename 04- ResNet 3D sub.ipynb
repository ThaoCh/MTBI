{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datautility import *\n",
    "from dataset import *\n",
    "from vnet import *\n",
    "from training import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using GPU for training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, sampler, SubsetRandomSampler\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import torch.nn.functional as F  # useful stateless functions\n",
    "import torchvision.transforms as T\n",
    "\n",
    "#------------------------------- GLOBAL VARIABLES -------------------------------------#\n",
    "\n",
    "USE_GPU = True\n",
    "BATCH_SIZE = 2\n",
    "NUM_WORKERS = 8\n",
    "NUM_TRAIN = 72 # 80 training sample and 37 validation sample\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('using GPU for training')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('using CPU for training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If Training new data only\n",
    "* Positive: 67 samples\n",
    "* Negative: 50 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TBI001', 'TBI002', 'TBI003', 'TBI004', 'TBI005', 'TBI006', 'TBI007', 'TBI008', 'TBI010', 'TBI011', 'TBI012', 'TBI013', 'TBI014', 'TBI015', 'TBI019', 'TBI020', 'TBI021', 'TBI022', 'TBI023', 'TBI024', 'TBI025', 'TBI026', 'TBI027', 'TBI028', 'TBI029', 'TBI031', 'TBI032', 'TBI033', 'TBI035', 'TBI036', 'TBI037', 'TBI038', 'TBI039', 'TBI040', 'TBI041', 'TBI042', 'TBI043', 'TBI044', 'TBI045', 'TBI046', 'TBI047', 'TBI048', 'TBI049', 'TBI050', 'TBI051', 'TBI052', 'TBI053', 'TBI054', 'TBI055', 'TBI056', 'TBI057', 'TBI058', 'TBI059', 'TBI060', 'TBI061', 'TBI062', 'TBI063', 'TBI064', 'TBI065', 'TBI066', 'TBI067', 'TBI068', 'TBI069', 'TBI070', 'TBI071', 'TBI072', 'TBI073', 'TBN001', 'TBN002', 'TBN003', 'TBN004', 'TBN005', 'TBN006', 'TBN007', 'TBN008', 'TBN009', 'TBN010', 'TBN011', 'TBN012', 'TBN013_2', 'TBN014_2', 'TBN018', 'TBN019', 'TBN020', 'TBN021', 'TBN022', 'TBN023', 'TBN024', 'TBN025', 'TBN026', 'TBN027', 'TBN028', 'TBN029', 'TBN030', 'TBN031', 'TBN032', 'TBN033', 'TBN034', 'TBN035', 'TBN036', 'TBN037', 'TBN038', 'TBN039', 'TBN040', 'TBN041', 'TBN042', 'TBN043', 'TBN044', 'TBN045', 'TBN046', 'TBN047', 'TBN048', 'TBN049', 'TBN050', 'TBN051', 'TBN052', 'TBN053'] 117\n"
     ]
    }
   ],
   "source": [
    "###################### Importing Packages #########################################\n",
    "# metric = ['ad', 'ak', 'awf', 'eas_De_par', 'eas_De_perp', 'eas_tort', 'FA', 'ias_Da', 'md', 'mk', 'rd', 'rk']\n",
    "# 12 metrics in total\n",
    "\n",
    "metric = ['ak', 'awf', 'eas_De_par', 'eas_De_perp', 'fa', 'ias_Da', 'md', 'mk']\n",
    "# 8 metric to use, proposed by Alp\n",
    "    \n",
    "NEW_PATH = './data/117_Stats_Rep.xls'\n",
    "\n",
    "new_data = pd.read_excel(NEW_PATH, header=0, index_col=0, sheet_name=0)\n",
    "new_idx_ori = list(new_data.index)\n",
    "\n",
    "new_idx = [i[3:-4] for i in new_idx_ori]\n",
    "\n",
    "print(new_idx, len(new_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regen = False\n",
    "\n",
    "if regen:\n",
    "    data_index = np.arange(117)\n",
    "    data_index = np.random.shuffle(data_index)\n",
    "    print(list(data_index))\n",
    "    \n",
    "else:\n",
    "    data_index = np.array ([27, 22, 83, 13, 57, 33, 53, 110, 58, 72, 12, 66, 69, 39, 105, 60, 42, 114, 91, 6, 80, 101,\n",
    "                            73, 87, 26, 74, 28, 19, 25, 79, 70, 1, 18, 2, 38, 99, 40, 48, 35, 20, 78, 102, 14, 54, 84,\n",
    "                            34, 77, 47, 9, 16, 59, 11, 112, 62, 71, 44, 85, 31, 51, 37, 94, 75, 21, 88, 68, 30, 104, 15,\n",
    "                            55, 61, 41, 115, 29, 49, 52, 107, 82, 45, 76, 89, 86, 92, 81, 95, 8, 116, 65, 106, 32, 56, 46,\n",
    "                            100, 36, 103, 90, 5, 43, 50, 67, 64, 111, 10, 63, 97, 17, 93, 98, 24, 7, 3, 0, 113, 108, 96, 4, 109, 23]\n",
    "                            )\n",
    "\n",
    "dataset_image = MTBIDatasetSub(new_index = new_idx,\n",
    "                            shuffle_index = data_index, \n",
    "                            metric = metric, \n",
    "                            transform=transforms.Compose([\n",
    "                                 RandomAffine(180, 15)\n",
    "                            ]),\n",
    "                     )\n",
    "\n",
    "#-------------------------CREATE DATA LOADER FOR TRAIN AND VAL------------------------#\n",
    "\n",
    "data_size = len(dataset_image)\n",
    "\n",
    "train_loader = DataLoader(dataset_image, batch_size=BATCH_SIZE, \\\n",
    "                    sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)),\\\n",
    "                    num_workers=NUM_WORKERS)\n",
    "validation_loader = DataLoader(dataset_image, batch_size=BATCH_SIZE,\n",
    "                    sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN,data_size)),\\\n",
    "                    num_workers=NUM_WORKERS)\n",
    "\n",
    "# data_set = MTBIDataset(image_dict, metric, transform=None, mode='new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([2, 8, 96, 96, 96]) torch.Size([2, 1])\n",
      "1 torch.Size([2, 8, 96, 96, 96]) torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "for i_batch, sample_batched in enumerate(train_loader):\n",
    "    print(i_batch, sample_batched['image'].size(), sample_batched['label'].size())\n",
    "    # observe 4th batch and stop.\n",
    "    if i_batch == 1:\n",
    "        # show_batch_image(sample_batched['image'],sample_batched['label'],BATCH_SIZE)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------NEW MODEL INIT WEIGHT--------------------------------------#\n",
    "\n",
    "LoadCKP = False\n",
    "CKPPath = 'checkpoint2019-03-31 13:33:50.772063.pth'\n",
    "\n",
    "model = LNet(img_size=(96, 96, 96))\n",
    "model.apply(weights_init)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=50, verbose=True)\n",
    "\n",
    "if LoadCKP:\n",
    "    model, optimizer, scheduler = loadckp(model, optimizer, scheduler, CKPPath, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 finished ! Training Loss: 0.9499, acc: 0.5556\n",
      "     validation loss = 3.5069, accuracy = 0.4783\n",
      "Checkpoint 1 saved !\n",
      "Epoch 1 finished ! Training Loss: 0.7202, acc: 0.6111\n",
      "     validation loss = 0.7215, accuracy = 0.5000\n",
      "Epoch 2 finished ! Training Loss: 0.8311, acc: 0.4444\n",
      "     validation loss = 0.7758, accuracy = 0.5000\n",
      "Epoch 3 finished ! Training Loss: 0.7445, acc: 0.5833\n",
      "     validation loss = 0.7297, accuracy = 0.5217\n",
      "Epoch 4 finished ! Training Loss: 0.7271, acc: 0.5556\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "train(model, train_loader, validation_loader, optimizer, scheduler, device, dtype, lossFun=loss, epochs=5000, streopch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
