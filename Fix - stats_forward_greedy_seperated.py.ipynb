{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import pickle, random, time\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import svm\n",
    "from itertools import chain, combinations\n",
    "from copy import deepcopy\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy SVM for Regional Statistic\n",
    "def greedy_forward_SVM(F, y, num_feature, num_selected, num_repeats, num_test, C_val, Gamma_val, eps_val):\n",
    "    '''\n",
    "    Args:\n",
    "        F: data\n",
    "        y: label\n",
    "        num_feature: number of feature in total. should be 120 but only 100\n",
    "        num_selected: number of feature selected\n",
    "        num_repeats: iter of repeat to avoid noise\n",
    "        num_test: number of test sample\n",
    "        C_val: c value as SVM hypter parameter\n",
    "        Gamma_val: gamma value as SVM hyper parameter\n",
    "        eps_val: eplison value as SVM hyper parameter\n",
    "    '''\n",
    "    best_acc_featsize = np.zeros((num_feature,))  ### accuracy with best subset of different sizes\n",
    "    all_feat_remained = np.arange(num_feature)\n",
    "    feat_order = list()\n",
    "\n",
    "    num_samples = F.shape[0]\n",
    "\n",
    "    for i in range(num_selected):  ## adds one feature per step\n",
    "\n",
    "        train_acc_cur = np.zeros((len(all_feat_remained), num_repeats))\n",
    "        test_acc_cur = np.zeros((len(all_feat_remained), num_repeats))\n",
    "        train_acc_avg = np.zeros((len(all_feat_remained),))\n",
    "        test_acc_avg = np.zeros((len(all_feat_remained),))\n",
    "        # print(\"%d-th feature selection\" % ( i+1) )\n",
    "\n",
    "        for j in range(len(all_feat_remained)):  ## selects one feature out of the remaining ones\n",
    "\n",
    "            cur_feat_list = deepcopy(feat_order)\n",
    "            cur_feat_list.append(all_feat_remained[j])\n",
    "            X = F[:, cur_feat_list]\n",
    "            # print(\"%d-th feature selection and feature list= [%s]\" % ( i+1, ', '.join(map(str, cur_feat_list))))\n",
    "\n",
    "            for iter in range(num_repeats):\n",
    "                # print(\"%d-th feature selection and %d-th iteration and feature list= [%s]\" % ( i+1, iter+1,', '.join(map(str, cur_feat_list))))\n",
    "\n",
    "                np.random.seed(3 * iter + 10)\n",
    "                inds = np.random.choice(len(y), num_test)\n",
    "\n",
    "                X_test = X[inds, :]\n",
    "                y_test = y[inds]\n",
    "                X_train = np.delete(X, inds, 0)\n",
    "                y_train = np.delete(y, inds)\n",
    "\n",
    "                clf = svm.SVR(C=C_val, epsilon=eps_val, kernel='rbf', degree=4, gamma=Gamma_val, tol=0.001,\n",
    "                              cache_size=200, max_iter=-1)\n",
    "                clf.fit(X_train, y_train)\n",
    "                predicted_train_labels = clf.predict(X_train)\n",
    "                train_score = clf.score(X_train, y_train)\n",
    "                # train_error= sum( [1. for k in pred_diff_train if k != 0])/len(predicted_train_labels)\n",
    "                train_acc_cur[j, iter] = train_score\n",
    "                predicted_test_labels = clf.predict(X_test)\n",
    "                test_score = clf.score(X_test, y_test)\n",
    "                # test_error= sum( [1. for k in pred_diff_test if k != 0])/len(predicted_test_labels)\n",
    "                test_acc_cur[j, iter] = test_score\n",
    "                # print(\"%d-th feature, current list [%s], and its acc= %f\" % ( j+1, ', '.join(map(str, cur_feat_list)), test_acc_cur[ j, iter] ))\n",
    "\n",
    "        for k in range(len(all_feat_remained)):\n",
    "            train_acc_avg[k] = np.mean(train_acc_cur[k, :])\n",
    "            test_acc_avg[k] = np.mean(test_acc_cur[k, :])\n",
    "\n",
    "        best_acc_featsize[i] = np.max(test_acc_avg)\n",
    "        best_testacc_ind = np.unravel_index(test_acc_avg.argmax(), test_acc_avg.shape)\n",
    "        feat_order.append(all_feat_remained[best_testacc_ind])\n",
    "        # print(\" current best feature index= %d\" %  (all_feat_remained[best_testacc_ind]) )\n",
    "\n",
    "        all_feat_remained = np.delete(all_feat_remained, best_testacc_ind, 0)\n",
    "\n",
    "    return feat_order, best_acc_featsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105, 100)\n",
      "(49, 100)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "F3 = pickle.load(open(\"./data/stats_data.p\", \"rb\"))\n",
    "# Reading stats_data\n",
    "\n",
    "labels_meso = np.zeros((45,)).astype(int)\n",
    "labels_old = np.hstack((np.ones((42,)), np.zeros((23,)))).astype(int)\n",
    "labels_117 = np.hstack((np.ones((67,)), np.zeros((50,)))).astype(int)\n",
    "\n",
    "F3_117 = F3[65:]\n",
    "F3_old = F3[:65]\n",
    "\n",
    "d117_delete_data = [2, 7, 14, 15, 30, 46, 47, 50, 54, 59, 72, 111]\n",
    "dold_delete_data = [1, 5, 6, 8, 10, 12, 14, 16, 21, 23, 26, 29, 33, 35, 37, 43]\n",
    "F3_117 = np.delete(F3_117, d117_delete_data, 0)\n",
    "print(F3_117.shape)\n",
    "F3_old = np.delete(F3_old, dold_delete_data, 0)\n",
    "print(F3_old.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_excel('./data/LUI_mTBI_NPwithZscores June 2018 cleaned up.xlsx')\n",
    "d117 = data1['WAIS_DigitSpanBack_z'].get_values()\n",
    "data2 = pd.read_excel(\n",
    "    './data/mTBI data first cycle Lui Grossman Miles 2018 review and update for combining.xlsx')\n",
    "dold = data2['Digit Span Backward z'].get_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105,)\n",
      "(49,)\n",
      "84\n",
      "using combined data for training\n",
      "Start\n",
      "( 0.001 , 1e-05 ) value of (C, Gamma), and best feat indices= [68 96 94 66 50 95 67 49 48  2]\n",
      "( 0.001 , 0.0017782794100389228 ) value of (C, Gamma), and best feat indices= [68 96 94 66 50 95 67 49 48  2]\n",
      "( 0.001 , 0.31622776601683794 ) value of (C, Gamma), and best feat indices= [68 67 22 49  0 21 93 16 19 17]\n",
      "( 0.001 , 56.23413251903491 ) value of (C, Gamma), and best feat indices= [92 15 16 31 93 17 19 18 70 69]\n",
      "( 0.001 , 10000.0 ) value of (C, Gamma), and best feat indices= [ 8  6  5  9  7  0  2 71 72  3]\n",
      "( 0.03162277660168379 , 1e-05 ) value of (C, Gamma), and best feat indices= [68 96 94 66 50 95 67 49 48  2]\n",
      "( 0.03162277660168379 , 0.0017782794100389228 ) value of (C, Gamma), and best feat indices= [68 96 94 66 50 95 67 48 49  2]\n",
      "( 0.03162277660168379 , 0.31622776601683794 ) value of (C, Gamma), and best feat indices= [68 21 96 20 95 22 67  5 23 74]\n",
      "( 0.03162277660168379 , 56.23413251903491 ) value of (C, Gamma), and best feat indices= [68 31 16 15 17 19 18 30 41 93]\n",
      "( 0.03162277660168379 , 10000.0 ) value of (C, Gamma), and best feat indices= [37 38 41 39 24 40 29 23 25 33]\n",
      "( 1.0 , 1e-05 ) value of (C, Gamma), and best feat indices= [68 96 94 66 50 95 67 48 49  2]\n",
      "( 1.0 , 0.0017782794100389228 ) value of (C, Gamma), and best feat indices= [68 66 96 50 67 94 35 48  2  3]\n",
      "( 1.0 , 0.31622776601683794 ) value of (C, Gamma), and best feat indices= [95 20 36 38 98 74 53 28 55 54]\n",
      "( 1.0 , 56.23413251903491 ) value of (C, Gamma), and best feat indices= [68 55  5 59 78 16 31 17 19  9]\n",
      "( 1.0 , 10000.0 ) value of (C, Gamma), and best feat indices= [93 47 40 41 37 25 38 29 31 10]\n",
      "( 31.622776601683793 , 1e-05 ) value of (C, Gamma), and best feat indices= [68 96 94 50 66 95 49 48  2  3]\n",
      "( 31.622776601683793 , 0.0017782794100389228 ) value of (C, Gamma), and best feat indices= [ 2 64 36 10 81  7 17 31 16 19]\n",
      "( 31.622776601683793 , 0.31622776601683794 ) value of (C, Gamma), and best feat indices= [98 20 26 14 40 54 82 36 39 13]\n",
      "( 31.622776601683793 , 56.23413251903491 ) value of (C, Gamma), and best feat indices= [ 5 31 15  9 16 71 11 79 17 19]\n",
      "( 31.622776601683793 , 10000.0 ) value of (C, Gamma), and best feat indices= [93 50 68 25 37 23 41 33 14 13]\n",
      "( 1000.0 , 1e-05 ) value of (C, Gamma), and best feat indices= [68 96 89 35 90 36 50  3 49  2]\n",
      "( 1000.0 , 0.0017782794100389228 ) value of (C, Gamma), and best feat indices= [36 45 25 64 95 96 34 71 88 16]\n",
      "( 1000.0 , 0.31622776601683794 ) value of (C, Gamma), and best feat indices= [98 21 55 16 19 41 17 15  9 54]\n",
      "( 1000.0 , 56.23413251903491 ) value of (C, Gamma), and best feat indices= [46 19 31 16  5  9  6 15 60 73]\n",
      "( 1000.0 , 10000.0 ) value of (C, Gamma), and best feat indices= [93  5 84 79 96  6 56 80 82 70]\n",
      "maximum test accuracy= 0.292, achieved by using 7 features and ( C, Gamma)= ( 1.000, 56.234) from 14 metrics\n",
      "Selected feature indices for 3xiter= [68. 55.  5. 59. 78. 16. 31.]\n",
      "total time= 3599.6207954883575\n"
     ]
    }
   ],
   "source": [
    "using_data = 'positive'\n",
    "\n",
    "delete_np_117 = [6, 13, 14, 15, 25, 29, 76, 77, 78]\n",
    "delete_np_old = [2, 6, 8, 18, 31, 32, 35, 37, 38]\n",
    "d117 = np.delete(d117, delete_np_117, 0)\n",
    "dold = np.delete(dold, delete_np_old, 0)\n",
    "\n",
    "print(d117.shape)\n",
    "print(dold.shape)\n",
    "\n",
    "labels_old = np.delete(labels_old, dold_delete_data, 0)\n",
    "labels_117 = np.delete(labels_117, d117_delete_data, 0)\n",
    "\n",
    "val = np.concatenate((d117, dold))\n",
    "\n",
    "L = np.hstack((labels_117, labels_old))\n",
    "\n",
    "print(np.count_nonzero(L))\n",
    "\n",
    "y = L.tolist()\n",
    "num_samples = len(L)\n",
    "\n",
    "# F2= np.delete( F2, unavailable_Npysch_ind, 0)\n",
    "# F3= np.delete( F3, unavailable_Npysch_ind, 0)\n",
    "# L= np.delete( L, unavailable_Npysch_ind, 0)\n",
    "\n",
    "F = np.concatenate((F3_117, F3_old))\n",
    "\n",
    "if using_data == 'postitive':\n",
    "    print(\"using positive data for training\")\n",
    "    val = val[L == 1]\n",
    "    F = F[L == 1, :]\n",
    "    L = L[L == 1]\n",
    "elif using_data== 'negative':\n",
    "    print(\"using negative data for training\")\n",
    "    val = val[L == 0]\n",
    "    F = F[L == 0, :]\n",
    "    L = L[L == 0]\n",
    "    print(len(L))\n",
    "    print(F.shape)\n",
    "    print(len(val))\n",
    "else:\n",
    "    print('using combined data for training')\n",
    "\n",
    "num_feature = F.shape[1]\n",
    "num_test = 20\n",
    "num_repeats = 25  ## shows number of times we shuffle the data and test on testing\n",
    "train_accuracies = [0] * num_repeats\n",
    "test_accuracies = [0] * num_repeats\n",
    "\n",
    "tot_num = F.shape[0]\n",
    "\n",
    "pos_ind = np.nonzero(L)[0]\n",
    "neg_ind = np.where(L == 0)[0]\n",
    "\n",
    "num_pos_training = np.int(np.floor((len(L) - num_test) * len(pos_ind) / (len(pos_ind) + len(neg_ind))))\n",
    "num_neg_training = np.int(len(L) - num_test - num_pos_training)\n",
    "\n",
    "# Best feature set in negative\n",
    "\n",
    "num_selected = 10\n",
    "eps = 0.05\n",
    "\n",
    "C_range = np.logspace(-3, 3.0, num=5)\n",
    "gamma_range = np.logspace(-5, 4, 5)\n",
    "\n",
    "test_acc_all = np.zeros((C_range.shape[0], gamma_range.shape[0], F.shape[1]))\n",
    "selected_feat = np.zeros((C_range.shape[0], gamma_range.shape[0], num_selected))\n",
    "\n",
    "print(\"Start\")\n",
    "for i in range(len(C_range)):\n",
    "    for j in range(len(gamma_range)):\n",
    "        a1, a2 = greedy_forward_SVM(F, val, num_feature, num_selected, num_repeats, num_test, C_range[i], gamma_range[j],\n",
    "                                    eps)\n",
    "        a3 = np.asarray(a1)\n",
    "        test_acc_all[i, j, :] = a2\n",
    "        selected_feat[i, j, :] = a3\n",
    "        print(\"(\", C_range[i], \",\", gamma_range[j], \") value of (C, Gamma), and best feat indices=\", a3)\n",
    "\n",
    "max_ind = np.unravel_index(test_acc_all.argmax(), test_acc_all.shape)\n",
    "print(\"maximum test accuracy= %.3f, achieved by using %d features and ( C, Gamma)= ( %.3f, %.3f) from 14 metrics\" % (\n",
    "test_acc_all[max_ind], max_ind[2] + 1, C_range[max_ind[0]], gamma_range[max_ind[1]]))\n",
    "print(\"Selected feature indices for 3xiter=\", selected_feat[max_ind[0], max_ind[1], 0:max_ind[2] + 1])\n",
    "\n",
    "# print(\"\\nMaximum Train Accuracy with 100 iteration: %f \" % np.max(train_acc_all), \"%\")\n",
    "# print(\"\\nMaximum Test Accuracy: %f, by feature indices= %s \" % ( np.max(test_acc_avg), subset_indices[best_testacc_ind[0]] ) )\n",
    "\n",
    "# print(\"\\nall accuracies:\", test_acc_avg)\n",
    "\n",
    "end_time = time.time()\n",
    "tot_time = end_time - start_time\n",
    "print(\"total time=\", tot_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IVP",
   "language": "python",
   "name": "cs231"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
