{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from datautility import *\n",
    "\n",
    "from scipy.io import loadmat, savemat\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TBI001', 'TBI002', 'TBI003', 'TBI004', 'TBI005', 'TBI006', 'TBI007', 'TBI008', 'TBI010', 'TBI011', 'TBI012', 'TBI013', 'TBI014', 'TBI015', 'TBI019', 'TBI020', 'TBI021', 'TBI022', 'TBI023', 'TBI024', 'TBI025', 'TBI026', 'TBI027', 'TBI028', 'TBI029', 'TBI031', 'TBI032', 'TBI033', 'TBI035', 'TBI036', 'TBI037', 'TBI038', 'TBI039', 'TBI040', 'TBI041', 'TBI042', 'TBI043', 'TBI044', 'TBI045', 'TBI046', 'TBI047', 'TBI048', 'TBI049', 'TBI050', 'TBI051', 'TBI052', 'TBI053', 'TBI054', 'TBI055', 'TBI056', 'TBI057', 'TBI058', 'TBI059', 'TBI060', 'TBI061', 'TBI062', 'TBI063', 'TBI064', 'TBI065', 'TBI066', 'TBI067', 'TBI068', 'TBI069', 'TBI070', 'TBI071', 'TBI072', 'TBI073', 'TBN001', 'TBN002', 'TBN003', 'TBN004', 'TBN005', 'TBN006', 'TBN007', 'TBN008', 'TBN009', 'TBN010', 'TBN011', 'TBN012', 'TBN013_2', 'TBN014_2', 'TBN018', 'TBN019', 'TBN020', 'TBN021', 'TBN022', 'TBN023', 'TBN024', 'TBN025', 'TBN026', 'TBN027', 'TBN028', 'TBN029', 'TBN030', 'TBN031', 'TBN032', 'TBN033', 'TBN034', 'TBN035', 'TBN036', 'TBN037', 'TBN038', 'TBN039', 'TBN040', 'TBN041', 'TBN042', 'TBN043', 'TBN044', 'TBN045', 'TBN046', 'TBN047', 'TBN048', 'TBN049', 'TBN050', 'TBN051', 'TBN052', 'TBN053'] 117\n"
     ]
    }
   ],
   "source": [
    "###################### Importing Packages #########################################\n",
    "# metric = ['ad', 'ak', 'awf', 'eas_De_par', 'eas_De_perp', 'eas_tort', 'FA', 'ias_Da', 'md', 'mk', 'rd', 'rk']\n",
    "# 12 metrics in total\n",
    "\n",
    "whichmask = 'old'\n",
    "\n",
    "metric = ['ak', 'awf', 'eas_De_par', 'eas_De_perp', 'FA', 'ias_Da', 'md', 'mk']\n",
    "# 8 metric to use, proposed by Alp\n",
    "if whichmask == 'new':\n",
    "    mask_name = ['left_rostral','right_rostral','left_middle','right_middle','left_caudal', 'right_caudal', 'corpus_callosum']\n",
    "# we do not have prefrontal mask for subject space\n",
    "else:\n",
    "    mask_name = ['cc_body', 'cc_genu', 'cc_splenium', 'thal']\n",
    "    \n",
    "NEW_PATH = './data/117_Stats_Rep.xls'\n",
    "\n",
    "new_data = pd.read_excel(NEW_PATH, header=0, index_col=0, sheet_name=0)\n",
    "new_idx_ori = list(new_data.index)\n",
    "\n",
    "new_idx = [i[3:-4] for i in new_idx_ori]\n",
    "# new_idx = new_idx[:-60] + new_idx[-50:-12] # filter mask currently\n",
    "\n",
    "print(new_idx, len(new_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unifying the '.nii' and '.mat'\n",
    "* Very annoying for a long time, also fix function get_image_subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = './data/current_cycle_subj_space_metrics_and_masks/metrics_mat/'\n",
    "old_path = './data/prior_cycle_subj_space_metrics_and_masks/metrics_mat/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(old_path): \n",
    "        if filename[-3:] == 'nii':\n",
    "            src = old_path + filename\n",
    "            dst = old_path + filename[:-3] + 'mat'\n",
    "            os.rename(src, dst) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(current_path): \n",
    "        if filename[-3:] == 'nii':\n",
    "            src = current_path + filename\n",
    "            dst = current_path + filename[:-3] + 'mat'\n",
    "            print(dst)\n",
    "            os.rename(src, dst) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting extra image and mask from nii to real mat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_folder = './data/extra/tbss/'\n",
    "\n",
    "for r, d, f in os.walk(extra_folder):\n",
    "    for file in f:\n",
    "        data_path = os.path.join(r, file)\n",
    "        data = ((nib.load(data_path)).get_fdata()).astype(np.float32)\n",
    "        data_dict = {}\n",
    "        data_dict['vol'] = data\n",
    "                \n",
    "        m = r[18:]\n",
    "        name = data_path[-10:-4]\n",
    "        if m == 'eas_De_par' or m == 'ias_Da' or m == 'awf' or m == 'eas_De_perp':\n",
    "            PATH = current_path + 'WMTI' + '_' + m + '_' + name + '.mat'\n",
    "        elif  m == 'ak':\n",
    "            PATH = current_path + 'DKI' + '_' + m + '_' + name + '.mat'\n",
    "        elif m == 'fa' or m =='md' or m =='mk':\n",
    "            PATH = current_path + 'DTI' + '_' + m + '_' + name + '.mat'\n",
    "        else:\n",
    "            PATH = None\n",
    "            print('metric {} is not implemented yet'.format(m))\n",
    "        \n",
    "        if PATH is not None:\n",
    "            savemat(PATH, data_dict, do_compression=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extra_mask = './data/extra/mask_nii/'\n",
    "current_mask = './data/current_cycle_subj_space_metrics_and_masks/mask_mat/'\n",
    "\n",
    "for r, d, f in os.walk(extra_mask):\n",
    "    for file in f:\n",
    "        \n",
    "        data_path = os.path.join(r, file)\n",
    "        data = ((nib.load(data_path)).get_fdata()).astype(np.float32)\n",
    "        data_dict = {}\n",
    "        data_dict['vol'] = data\n",
    "        \n",
    "        PATH = current_mask + file[:-3] + 'mat'\n",
    "        savemat(PATH, data_dict, do_compression=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name in tqdm(new_idx):\n",
    "    data = get_image_subject(name, metric, 'new', shape=(96, 96, 96), verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'TBI001'\n",
    "print(test[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_slice(data, N):\n",
    "    '''\n",
    "    data: 3d ndarray\n",
    "    N: number of slice taken at X axis\n",
    "    '''\n",
    "    slices = np.round (np.linspace (0, data.shape[0], N, endpoint=False)).astype(np.int16)\n",
    "    fig, ax = plt.subplots(1, N, figsize=(15, 4), sharey=True)\n",
    "\n",
    "    for i,s in enumerate(slices):\n",
    "        ax[i].imshow(data[s], cmap='gray')\n",
    "        \n",
    "def show_negative(index, metric):\n",
    "    for m in metric:\n",
    "        PATH = './data/current_cycle_117subj_all_metrics/mat/'+m+'_TBI'+'{:03}'.format(index)+'.mat'\n",
    "        data = loadmat(PATH)\n",
    "        print(data['vol'].shape)\n",
    "        show_slice(data['vol'], N=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metric = ['ad', 'ak', 'awf', 'eas_De_par', 'eas_De_perp', 'eas_tort', 'FA', 'ias_Da', 'md', 'mk', 'rd', 'rk']\n",
    "# show_negative(1, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img0 = loadmat('./data/masks/CC_Splenium_mask.mat')['vol']\n",
    "img1 = loadmat('./data/masks/CC_Genu_mask.mat')['vol']\n",
    "img2 = loadmat('./data/masks/CC_Body_mask.mat')['vol']\n",
    "img3 = loadmat('./data/masks/2_R_thal.mat')['vol']\n",
    "img4 = loadmat('./data/masks/1_L_thal.mat')['vol']\n",
    "\n",
    "print(img0.shape)\n",
    "\n",
    "img = img0 + img1*2 + img2*3 + img3*4 + img4*5\n",
    "\n",
    "show_slice(img, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. View the Masks\n",
    "* JHU-ICBM-labels-1mm_CortexWM \n",
    "* JHU-ICBM-labels-1mm_7ROI\n",
    "* Bilateral_prefrontalWM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './data/masks'\n",
    "APPEND = '.nii'\n",
    "FNAME = ['JHU-ICBM-labels-1mm_CortexWM', 'JHU-ICBM-labels-1mm_7ROI', 'Bilateral_prefrontalWM']\n",
    "\n",
    "for f in FNAME:\n",
    "    data_file = os.path.join(PATH, f + APPEND)\n",
    "    data = ((nib.load(data_file)).get_fdata()).astype(np.float32)\n",
    "    data = np.squeeze(data)\n",
    "    \n",
    "    N = 8\n",
    "    slices = np.round (np.linspace (0, data.shape[0], N, endpoint=False)).astype(np.int16)\n",
    "    fig, ax = plt.subplots(1, N, figsize=(15, 4), sharey=True)\n",
    "    \n",
    "    for i,s in enumerate(slices):\n",
    "        ax[i].imshow(data[s], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating Statistics for Regions into DataFrame\n",
    "* For the old data, 7 regions are extracted: L_Thal, R_Thal, CC_Genu, CC_Body, CC_Splenium, L_PrefrontalWM, R_PrefrontalWM\n",
    "* For the new data, 5 regions are extracted: L_Thal, R_Thal, CC_Genu, CC_Body, CC_Splenium\n",
    "* FA, AD, AK, AWF, Depar, Deparp, TORT, DA, MD, MK, RD, RK are extracted from original data\n",
    "* FA, MD, MK, AWF, AK, Depar, Deperp, DA are the metrics\n",
    "* Save to STA_PATH = './data/stats_data_fix.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Setting up the Path and Keys ########################################\n",
    "\n",
    "OLD_PATH = './data/old_65subj_stats JFR.xlsx'\n",
    "NEW_PATH = './data/117_Stats_Rep.xlsx'\n",
    "\n",
    "old_data = pd.read_excel(OLD_PATH, header=0, index_col=0, sheet_name=None)\n",
    "new_data = pd.read_excel(NEW_PATH, header=0, index_col=0, sheet_name=None)\n",
    "\n",
    "old_keys = ['all_FA_mean', 'all_FA_std',  # FA\n",
    "            'all_ad_uthr6_mean', 'all_ad_uthr6_std', # AD\n",
    "            'all_ak_mean', 'all_ak_std',  # AK\n",
    "            'all_awf_mean', 'all_awf_std', # AWF\n",
    "            'all_eas_de_par_uthr6_mean', 'all_eas_de_par_uthr6_std', # DEPAR \n",
    "            'all_eas_de_perp_uthr6_mean', 'all_eas_de_perp_uthr6_std',  # DEPERP\n",
    "            'all_eas_tort_uthr9_mean', 'all_eas_tort_uthr9_std', # TORT\n",
    "            'all_ias_Da_uthr5_mean', 'all_ias_Da_uthr5_std', # DA\n",
    "            'all_md_uthr5_mean', 'all_md_uthr5_std', # MD\n",
    "            'all_mk_uthr5_mean', 'all_mk_uthr5_std', # MK\n",
    "            'all_rd_uthr5_mean', 'all_rd_uthr5_std', # RD\n",
    "            'all_rk_uthr5_mean', 'all_rk_uthr5_std' # RK\n",
    "]\n",
    "\n",
    "new_keys = ['all_FA_mean', 'all_FA_std', # FA\n",
    "            'all_ad_uthr6_mean', 'all_ad_uthr6_std', # AD\n",
    "            'all_ak_mean', 'all_ak_std', # AK\n",
    "            'all_awf_mean', 'all_awf_std', # AWF\n",
    "            'all_eas_De_par_mean', 'all_eas_De_par_std', # DEPAR\n",
    "            'all_eas_De_perp_mean', 'all_eas_De_perp_std', # DEPREP\n",
    "            'all_tort_mean', 'all_tort_std', # TORT\n",
    "            'all_Da_mean', 'all_Da_std', # DA\n",
    "            'all_md_mean', 'all_md_std', # MD\n",
    "            'all_mk_mean', 'all_mk_std', # MK\n",
    "            'all_rd_mean', 'all_rd_std', # RD\n",
    "            'all_rk_mean', 'all_rk_std', # RK\n",
    "]\n",
    "\n",
    "mask_keys = ['L_Thal', 'R_Thal', 'CC_Genu', 'CC_Body', 'CC_Splenium']\n",
    "\n",
    "####################### Rescheduling the columns and row label ##############################\n",
    "\n",
    "Index_column = ['-'.join(i) for i in list(itertools.product(new_keys, mask_keys))]\n",
    "\n",
    "data_TBI_index = np.arange(start=1, stop=74)\n",
    "data_TBN_index = np.arange(start=1, stop=54)\n",
    "\n",
    "data_TBI_index = np.delete(data_TBI_index, [8,15,16,17,29,33])\n",
    "data_TBN_index = np.delete(data_TBN_index, [14, 15, 16])\n",
    "\n",
    "Index_TBI = ['TBI-' + '{:03}'.format(i) for i in data_TBI_index]\n",
    "Index_TBN = ['TBN-' + '{:03}'.format(i) for i in data_TBN_index]\n",
    "\n",
    "Index_row = Index_TBI + Index_TBN + list (old_data['all_FA_mean'].index)\n",
    "\n",
    "data_list = [] # Final Data List\n",
    "\n",
    "####################### Combining old data with the news ####################################\n",
    "\n",
    "for okey, nkey in zip(old_keys, new_keys):\n",
    "    \n",
    "    old_temp = old_data[okey].iloc[:, 0:5].values\n",
    "    new_temp = new_data[nkey].values\n",
    "    \n",
    "    temp = np.vstack([new_temp, old_temp]) # new on top of old\n",
    "    data_list.append(temp)\n",
    "\n",
    "data = np.hstack(data_list)\n",
    "print('in total {} subjects with {} of features each'.format(*data.shape))\n",
    "\n",
    "data_dic = pd.DataFrame(data=data, index=Index_row, columns=Index_column)\n",
    "\n",
    "####################### Save dataframe to Excel #############################################\n",
    "\n",
    "STA_PATH = './data/stats_data.xlsx'\n",
    "if not os.path.isfile(STA_PATH):\n",
    "    data_dic.to_excel(STA_PATH)\n",
    "else:\n",
    "    print('file {} exists'.format(STA_PATH))\n",
    "    \n",
    "print(data_dic.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Combining Data with the Ground Truth\n",
    "* In total 154 subs have both data and at least one label\n",
    "* first 120 columns are features, the rest are labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Setting up the Path and Keys ########################################\n",
    "\n",
    "DATA_PATH = './data/stats_data.xlsx'\n",
    "LABEL_PATH = './data/merged_np.xlsx'\n",
    "\n",
    "data_df = pd.read_excel(DATA_PATH, index_col=0)\n",
    "label_df = pd.read_excel(LABEL_PATH,  sheet_name=1, index_col=1)\n",
    "\n",
    "sub_data = np.array (list (data_df.index))\n",
    "sub_label = np.array (list (label_df.index))\n",
    "\n",
    "haveSubject = (sub_data[None,:] == sub_label[:,None]).astype(np.int16)\n",
    "\n",
    "sub_index = []\n",
    "\n",
    "####################### Finding the subjects shared by data and label #######################\n",
    "\n",
    "for i in range (haveSubject.shape[1]):\n",
    "    if np.sum(haveSubject[:,i]):\n",
    "        data_idx = i\n",
    "        label_idx = np.argmax(haveSubject[:,i])\n",
    "        sub_index.append((data_idx, label_idx))\n",
    "\n",
    "sub_index_data = [i[0] for i in sub_index]\n",
    "sub_index_label = [i[1] for i in sub_index]        \n",
    "\n",
    "data_df = data_df.iloc[sub_index_data, :]\n",
    "label_df = label_df.iloc[sub_index_label,:]\n",
    "\n",
    "whole_data = pd.concat([data_df, label_df], axis=1)\n",
    "\n",
    "####################### Save dataframe to Excel #############################################\n",
    "\n",
    "WDTA_PATH = './data/stats_data_label.xlsx'\n",
    "\n",
    "if not os.path.isfile(WDTA_PATH):\n",
    "    whole_data.to_excel(WDTA_PATH)\n",
    "else:\n",
    "    print('file {} exists'.format(WDTA_PATH))\n",
    "    \n",
    "print(whole_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flitered = whole_data [whole_data['T1 Letter Number'].notnull()]\n",
    "X = data_flitered.values [:,0:120]\n",
    "Y = data_flitered.values [:,121:122] # T1 Letter Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtr, Xts, Ytr, Yts = train_test_split(X, Y.ravel(), test_size=0.333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "regr = Lasso()\n",
    "cv = KFold(n_splits=5, shuffle=True)\n",
    "parameters = { 'alpha':np.arange(100)/50}\n",
    "\n",
    "regr_grid = GridSearchCV(regr, parameters, cv = cv, scoring= 'r2', n_jobs=-1, verbose=10)\n",
    "regr_grid.fit(X, Y)\n",
    "print(regr_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor()\n",
    "\n",
    "parameters = { 'min_samples_leaf':[1,2,4,8,16],'max_depth':[2,4,8,16],'n_estimators':[500]}\n",
    "model_grid = GridSearchCV(model, parameters, cv = cv, scoring= 'r2', n_jobs=-1, verbose=10)\n",
    "\n",
    "model_grid.fit(X, Y)\n",
    "print(model_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "Ypred = regr.predict(Xts)\n",
    "print(mean_squared_error(Yts, Ypred), r2_score(Yts, Ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_flitered.values [:,idx_sel]\n",
    "Y = data_flitered.values [:,121:122] # T1 Letter Number\n",
    "\n",
    "Xtr, Xts, Ytr, Yts = train_test_split(X, Y.ravel(), test_size=0.333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "parameters = { 'min_samples_leaf':[5,10],'max_depth':[10],'n_estimators':[500]}\n",
    "model_grid = GridSearchCV(model, parameters, cv = 5, scoring= 'r2', n_jobs=-1, verbose=10)\n",
    "\n",
    "model_grid.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = RandomForestRegressor(max_depth=10,min_samples_leaf=5, random_state=0, n_estimators=500)\n",
    "regr.fit(Xtr, Ytr)\n",
    "\n",
    "Ytrhat = regr.predict(Xtr)\n",
    "Ypred = regr.predict(Xts)\n",
    "print(mean_squared_error(Ytr, Ytrhat), r2_score(Ytr, Ytrhat))\n",
    "print(mean_squared_error(Yts, Ypred), r2_score(Yts, Ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STA_PATH = './data/stats_data.p'\n",
    "stat = pickle.load(open(STA_PATH, \"rb\"))\n",
    "\n",
    "print(stat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IVP",
   "language": "python",
   "name": "cs231"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
